{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d385c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "636b59eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic regression data\n",
    "X, y_reg = make_regression(n_samples=1000, n_features=10, noise=0.1)\n",
    "y_class = (y_reg > y_reg.mean()).astype(int)  # Binary classification label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "882168e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split \n",
    "X_train, X_test, y_reg_train, y_reg_test, y_class_train, y_class_test = train_test_split(X, y_reg, y_class, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7649526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)  \n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_reg_train_tensor = torch.tensor(y_reg_train, dtype=torch.float32).view(-1, 1)\n",
    "y_reg_test_tensor = torch.tensor(y_reg_test, dtype=torch.float32).view(-1, 1)\n",
    "y_class_train_tensor = torch.tensor(y_class_train, dtype=torch.float32).view(-1, 1)\n",
    "y_class_test_tensor = torch.tensor(y_class_test, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab4aee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model building\n",
    "class MultiTaskNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.regression_head = nn.Linear(32, 1)\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared(x)\n",
    "        reg_out = self.regression_head(shared_out)\n",
    "        class_out = self.classification_head(shared_out)\n",
    "        return reg_out, class_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efb947e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training and instatiation\n",
    "model = MultiTaskNet(input_size=X_train_tensor.shape[1])\n",
    "criterion_reg = nn.MSELoss()\n",
    "critreion_class = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9bb4e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10000], Loss: 56.8785\n",
      "Epoch [20/10000], Loss: 55.9209\n",
      "Epoch [30/10000], Loss: 54.9741\n",
      "Epoch [40/10000], Loss: 54.0382\n",
      "Epoch [50/10000], Loss: 53.1139\n",
      "Epoch [60/10000], Loss: 52.1986\n",
      "Epoch [70/10000], Loss: 51.2918\n",
      "Epoch [80/10000], Loss: 50.3972\n",
      "Epoch [90/10000], Loss: 49.5123\n",
      "Epoch [100/10000], Loss: 48.6395\n",
      "Epoch [110/10000], Loss: 47.7825\n",
      "Epoch [120/10000], Loss: 46.9389\n",
      "Epoch [130/10000], Loss: 46.1090\n",
      "Epoch [140/10000], Loss: 45.2917\n",
      "Epoch [150/10000], Loss: 44.4858\n",
      "Epoch [160/10000], Loss: 43.6918\n",
      "Epoch [170/10000], Loss: 42.9081\n",
      "Epoch [180/10000], Loss: 42.1354\n",
      "Epoch [190/10000], Loss: 41.3760\n",
      "Epoch [200/10000], Loss: 40.6268\n",
      "Epoch [210/10000], Loss: 39.8894\n",
      "Epoch [220/10000], Loss: 39.1636\n",
      "Epoch [230/10000], Loss: 38.4451\n",
      "Epoch [240/10000], Loss: 37.7361\n",
      "Epoch [250/10000], Loss: 37.0387\n",
      "Epoch [260/10000], Loss: 36.3505\n",
      "Epoch [270/10000], Loss: 35.6717\n",
      "Epoch [280/10000], Loss: 35.0028\n",
      "Epoch [290/10000], Loss: 34.3465\n",
      "Epoch [300/10000], Loss: 33.7027\n",
      "Epoch [310/10000], Loss: 33.0709\n",
      "Epoch [320/10000], Loss: 32.4500\n",
      "Epoch [330/10000], Loss: 31.8393\n",
      "Epoch [340/10000], Loss: 31.2408\n",
      "Epoch [350/10000], Loss: 30.6543\n",
      "Epoch [360/10000], Loss: 30.0782\n",
      "Epoch [370/10000], Loss: 29.5110\n",
      "Epoch [380/10000], Loss: 28.9537\n",
      "Epoch [390/10000], Loss: 28.4055\n",
      "Epoch [400/10000], Loss: 27.8671\n",
      "Epoch [410/10000], Loss: 27.3402\n",
      "Epoch [420/10000], Loss: 26.8241\n",
      "Epoch [430/10000], Loss: 26.3182\n",
      "Epoch [440/10000], Loss: 25.8233\n",
      "Epoch [450/10000], Loss: 25.3378\n",
      "Epoch [460/10000], Loss: 24.8620\n",
      "Epoch [470/10000], Loss: 24.3865\n",
      "Epoch [480/10000], Loss: 23.9150\n",
      "Epoch [490/10000], Loss: 23.4530\n",
      "Epoch [500/10000], Loss: 22.9984\n",
      "Epoch [510/10000], Loss: 22.5502\n",
      "Epoch [520/10000], Loss: 22.1086\n",
      "Epoch [530/10000], Loss: 21.6723\n",
      "Epoch [540/10000], Loss: 21.2457\n",
      "Epoch [550/10000], Loss: 20.8302\n",
      "Epoch [560/10000], Loss: 20.4224\n",
      "Epoch [570/10000], Loss: 20.0238\n",
      "Epoch [580/10000], Loss: 19.6352\n",
      "Epoch [590/10000], Loss: 19.2549\n",
      "Epoch [600/10000], Loss: 18.8852\n",
      "Epoch [610/10000], Loss: 18.5242\n",
      "Epoch [620/10000], Loss: 18.1690\n",
      "Epoch [630/10000], Loss: 17.8225\n",
      "Epoch [640/10000], Loss: 17.4855\n",
      "Epoch [650/10000], Loss: 17.1575\n",
      "Epoch [660/10000], Loss: 16.8378\n",
      "Epoch [670/10000], Loss: 16.5268\n",
      "Epoch [680/10000], Loss: 16.2238\n",
      "Epoch [690/10000], Loss: 15.9305\n",
      "Epoch [700/10000], Loss: 15.6471\n",
      "Epoch [710/10000], Loss: 15.3718\n",
      "Epoch [720/10000], Loss: 15.1045\n",
      "Epoch [730/10000], Loss: 14.8453\n",
      "Epoch [740/10000], Loss: 14.5916\n",
      "Epoch [750/10000], Loss: 14.3410\n",
      "Epoch [760/10000], Loss: 14.0953\n",
      "Epoch [770/10000], Loss: 13.8552\n",
      "Epoch [780/10000], Loss: 13.6193\n",
      "Epoch [790/10000], Loss: 13.3880\n",
      "Epoch [800/10000], Loss: 13.1607\n",
      "Epoch [810/10000], Loss: 12.9394\n",
      "Epoch [820/10000], Loss: 12.7237\n",
      "Epoch [830/10000], Loss: 12.5114\n",
      "Epoch [840/10000], Loss: 12.3030\n",
      "Epoch [850/10000], Loss: 12.0969\n",
      "Epoch [860/10000], Loss: 11.8956\n",
      "Epoch [870/10000], Loss: 11.6970\n",
      "Epoch [880/10000], Loss: 11.5009\n",
      "Epoch [890/10000], Loss: 11.3094\n",
      "Epoch [900/10000], Loss: 11.1236\n",
      "Epoch [910/10000], Loss: 10.9428\n",
      "Epoch [920/10000], Loss: 10.7681\n",
      "Epoch [930/10000], Loss: 10.5968\n",
      "Epoch [940/10000], Loss: 10.4308\n",
      "Epoch [950/10000], Loss: 10.2695\n",
      "Epoch [960/10000], Loss: 10.1122\n",
      "Epoch [970/10000], Loss: 9.9597\n",
      "Epoch [980/10000], Loss: 9.8117\n",
      "Epoch [990/10000], Loss: 9.6677\n",
      "Epoch [1000/10000], Loss: 9.5278\n",
      "Epoch [1010/10000], Loss: 9.3910\n",
      "Epoch [1020/10000], Loss: 9.2573\n",
      "Epoch [1030/10000], Loss: 9.1238\n",
      "Epoch [1040/10000], Loss: 8.9922\n",
      "Epoch [1050/10000], Loss: 8.8633\n",
      "Epoch [1060/10000], Loss: 8.7375\n",
      "Epoch [1070/10000], Loss: 8.6152\n",
      "Epoch [1080/10000], Loss: 8.4962\n",
      "Epoch [1090/10000], Loss: 8.3801\n",
      "Epoch [1100/10000], Loss: 8.2670\n",
      "Epoch [1110/10000], Loss: 8.1570\n",
      "Epoch [1120/10000], Loss: 8.0498\n",
      "Epoch [1130/10000], Loss: 7.9452\n",
      "Epoch [1140/10000], Loss: 7.8429\n",
      "Epoch [1150/10000], Loss: 7.7421\n",
      "Epoch [1160/10000], Loss: 7.6434\n",
      "Epoch [1170/10000], Loss: 7.5464\n",
      "Epoch [1180/10000], Loss: 7.4512\n",
      "Epoch [1190/10000], Loss: 7.3583\n",
      "Epoch [1200/10000], Loss: 7.2675\n",
      "Epoch [1210/10000], Loss: 7.1782\n",
      "Epoch [1220/10000], Loss: 7.0904\n",
      "Epoch [1230/10000], Loss: 7.0046\n",
      "Epoch [1240/10000], Loss: 6.9211\n",
      "Epoch [1250/10000], Loss: 6.8392\n",
      "Epoch [1260/10000], Loss: 6.7586\n",
      "Epoch [1270/10000], Loss: 6.6791\n",
      "Epoch [1280/10000], Loss: 6.6014\n",
      "Epoch [1290/10000], Loss: 6.5258\n",
      "Epoch [1300/10000], Loss: 6.4515\n",
      "Epoch [1310/10000], Loss: 6.3787\n",
      "Epoch [1320/10000], Loss: 6.3076\n",
      "Epoch [1330/10000], Loss: 6.2382\n",
      "Epoch [1340/10000], Loss: 6.1697\n",
      "Epoch [1350/10000], Loss: 6.1022\n",
      "Epoch [1360/10000], Loss: 6.0360\n",
      "Epoch [1370/10000], Loss: 5.9710\n",
      "Epoch [1380/10000], Loss: 5.9073\n",
      "Epoch [1390/10000], Loss: 5.8447\n",
      "Epoch [1400/10000], Loss: 5.7827\n",
      "Epoch [1410/10000], Loss: 5.7219\n",
      "Epoch [1420/10000], Loss: 5.6624\n",
      "Epoch [1430/10000], Loss: 5.6045\n",
      "Epoch [1440/10000], Loss: 5.5476\n",
      "Epoch [1450/10000], Loss: 5.4917\n",
      "Epoch [1460/10000], Loss: 5.4365\n",
      "Epoch [1470/10000], Loss: 5.3813\n",
      "Epoch [1480/10000], Loss: 5.3263\n",
      "Epoch [1490/10000], Loss: 5.2726\n",
      "Epoch [1500/10000], Loss: 5.2197\n",
      "Epoch [1510/10000], Loss: 5.1678\n",
      "Epoch [1520/10000], Loss: 5.1165\n",
      "Epoch [1530/10000], Loss: 5.0658\n",
      "Epoch [1540/10000], Loss: 5.0164\n",
      "Epoch [1550/10000], Loss: 4.9676\n",
      "Epoch [1560/10000], Loss: 4.9192\n",
      "Epoch [1570/10000], Loss: 4.8721\n",
      "Epoch [1580/10000], Loss: 4.8257\n",
      "Epoch [1590/10000], Loss: 4.7802\n",
      "Epoch [1600/10000], Loss: 4.7356\n",
      "Epoch [1610/10000], Loss: 4.6916\n",
      "Epoch [1620/10000], Loss: 4.6479\n",
      "Epoch [1630/10000], Loss: 4.6050\n",
      "Epoch [1640/10000], Loss: 4.5629\n",
      "Epoch [1650/10000], Loss: 4.5214\n",
      "Epoch [1660/10000], Loss: 4.4802\n",
      "Epoch [1670/10000], Loss: 4.4393\n",
      "Epoch [1680/10000], Loss: 4.3980\n",
      "Epoch [1690/10000], Loss: 4.3568\n",
      "Epoch [1700/10000], Loss: 4.3164\n",
      "Epoch [1710/10000], Loss: 4.2764\n",
      "Epoch [1720/10000], Loss: 4.2367\n",
      "Epoch [1730/10000], Loss: 4.1966\n",
      "Epoch [1740/10000], Loss: 4.1565\n",
      "Epoch [1750/10000], Loss: 4.1163\n",
      "Epoch [1760/10000], Loss: 4.0765\n",
      "Epoch [1770/10000], Loss: 4.0370\n",
      "Epoch [1780/10000], Loss: 3.9976\n",
      "Epoch [1790/10000], Loss: 3.9586\n",
      "Epoch [1800/10000], Loss: 3.9202\n",
      "Epoch [1810/10000], Loss: 3.8823\n",
      "Epoch [1820/10000], Loss: 3.8443\n",
      "Epoch [1830/10000], Loss: 3.8060\n",
      "Epoch [1840/10000], Loss: 3.7681\n",
      "Epoch [1850/10000], Loss: 3.7302\n",
      "Epoch [1860/10000], Loss: 3.6929\n",
      "Epoch [1870/10000], Loss: 3.6558\n",
      "Epoch [1880/10000], Loss: 3.6187\n",
      "Epoch [1890/10000], Loss: 3.5823\n",
      "Epoch [1900/10000], Loss: 3.5464\n",
      "Epoch [1910/10000], Loss: 3.5113\n",
      "Epoch [1920/10000], Loss: 3.4772\n",
      "Epoch [1930/10000], Loss: 3.4438\n",
      "Epoch [1940/10000], Loss: 3.4111\n",
      "Epoch [1950/10000], Loss: 3.3791\n",
      "Epoch [1960/10000], Loss: 3.3479\n",
      "Epoch [1970/10000], Loss: 3.3175\n",
      "Epoch [1980/10000], Loss: 3.2874\n",
      "Epoch [1990/10000], Loss: 3.2581\n",
      "Epoch [2000/10000], Loss: 3.2292\n",
      "Epoch [2010/10000], Loss: 3.2005\n",
      "Epoch [2020/10000], Loss: 3.1722\n",
      "Epoch [2030/10000], Loss: 3.1442\n",
      "Epoch [2040/10000], Loss: 3.1162\n",
      "Epoch [2050/10000], Loss: 3.0886\n",
      "Epoch [2060/10000], Loss: 3.0611\n",
      "Epoch [2070/10000], Loss: 3.0341\n",
      "Epoch [2080/10000], Loss: 3.0074\n",
      "Epoch [2090/10000], Loss: 2.9810\n",
      "Epoch [2100/10000], Loss: 2.9544\n",
      "Epoch [2110/10000], Loss: 2.9273\n",
      "Epoch [2120/10000], Loss: 2.9003\n",
      "Epoch [2130/10000], Loss: 2.8735\n",
      "Epoch [2140/10000], Loss: 2.8467\n",
      "Epoch [2150/10000], Loss: 2.8205\n",
      "Epoch [2160/10000], Loss: 2.7951\n",
      "Epoch [2170/10000], Loss: 2.7704\n",
      "Epoch [2180/10000], Loss: 2.7458\n",
      "Epoch [2190/10000], Loss: 2.7218\n",
      "Epoch [2200/10000], Loss: 2.6983\n",
      "Epoch [2210/10000], Loss: 2.6753\n",
      "Epoch [2220/10000], Loss: 2.6525\n",
      "Epoch [2230/10000], Loss: 2.6301\n",
      "Epoch [2240/10000], Loss: 2.6080\n",
      "Epoch [2250/10000], Loss: 2.5862\n",
      "Epoch [2260/10000], Loss: 2.5646\n",
      "Epoch [2270/10000], Loss: 2.5433\n",
      "Epoch [2280/10000], Loss: 2.5223\n",
      "Epoch [2290/10000], Loss: 2.5010\n",
      "Epoch [2300/10000], Loss: 2.4795\n",
      "Epoch [2310/10000], Loss: 2.4579\n",
      "Epoch [2320/10000], Loss: 2.4365\n",
      "Epoch [2330/10000], Loss: 2.4155\n",
      "Epoch [2340/10000], Loss: 2.3949\n",
      "Epoch [2350/10000], Loss: 2.3746\n",
      "Epoch [2360/10000], Loss: 2.3545\n",
      "Epoch [2370/10000], Loss: 2.3345\n",
      "Epoch [2380/10000], Loss: 2.3147\n",
      "Epoch [2390/10000], Loss: 2.2950\n",
      "Epoch [2400/10000], Loss: 2.2756\n",
      "Epoch [2410/10000], Loss: 2.2564\n",
      "Epoch [2420/10000], Loss: 2.2375\n",
      "Epoch [2430/10000], Loss: 2.2189\n",
      "Epoch [2440/10000], Loss: 2.2007\n",
      "Epoch [2450/10000], Loss: 2.1828\n",
      "Epoch [2460/10000], Loss: 2.1648\n",
      "Epoch [2470/10000], Loss: 2.1468\n",
      "Epoch [2480/10000], Loss: 2.1293\n",
      "Epoch [2490/10000], Loss: 2.1120\n",
      "Epoch [2500/10000], Loss: 2.0949\n",
      "Epoch [2510/10000], Loss: 2.0779\n",
      "Epoch [2520/10000], Loss: 2.0608\n",
      "Epoch [2530/10000], Loss: 2.0433\n",
      "Epoch [2540/10000], Loss: 2.0258\n",
      "Epoch [2550/10000], Loss: 2.0085\n",
      "Epoch [2560/10000], Loss: 1.9914\n",
      "Epoch [2570/10000], Loss: 1.9747\n",
      "Epoch [2580/10000], Loss: 1.9581\n",
      "Epoch [2590/10000], Loss: 1.9419\n",
      "Epoch [2600/10000], Loss: 1.9258\n",
      "Epoch [2610/10000], Loss: 1.9099\n",
      "Epoch [2620/10000], Loss: 1.8942\n",
      "Epoch [2630/10000], Loss: 1.8788\n",
      "Epoch [2640/10000], Loss: 1.8636\n",
      "Epoch [2650/10000], Loss: 1.8482\n",
      "Epoch [2660/10000], Loss: 1.8329\n",
      "Epoch [2670/10000], Loss: 1.8179\n",
      "Epoch [2680/10000], Loss: 1.8032\n",
      "Epoch [2690/10000], Loss: 1.7889\n",
      "Epoch [2700/10000], Loss: 1.7749\n",
      "Epoch [2710/10000], Loss: 1.7611\n",
      "Epoch [2720/10000], Loss: 1.7476\n",
      "Epoch [2730/10000], Loss: 1.7342\n",
      "Epoch [2740/10000], Loss: 1.7209\n",
      "Epoch [2750/10000], Loss: 1.7076\n",
      "Epoch [2760/10000], Loss: 1.6946\n",
      "Epoch [2770/10000], Loss: 1.6816\n",
      "Epoch [2780/10000], Loss: 1.6686\n",
      "Epoch [2790/10000], Loss: 1.6556\n",
      "Epoch [2800/10000], Loss: 1.6429\n",
      "Epoch [2810/10000], Loss: 1.6305\n",
      "Epoch [2820/10000], Loss: 1.6181\n",
      "Epoch [2830/10000], Loss: 1.6059\n",
      "Epoch [2840/10000], Loss: 1.5938\n",
      "Epoch [2850/10000], Loss: 1.5820\n",
      "Epoch [2860/10000], Loss: 1.5703\n",
      "Epoch [2870/10000], Loss: 1.5589\n",
      "Epoch [2880/10000], Loss: 1.5477\n",
      "Epoch [2890/10000], Loss: 1.5366\n",
      "Epoch [2900/10000], Loss: 1.5257\n",
      "Epoch [2910/10000], Loss: 1.5149\n",
      "Epoch [2920/10000], Loss: 1.5041\n",
      "Epoch [2930/10000], Loss: 1.4929\n",
      "Epoch [2940/10000], Loss: 1.4816\n",
      "Epoch [2950/10000], Loss: 1.4704\n",
      "Epoch [2960/10000], Loss: 1.4592\n",
      "Epoch [2970/10000], Loss: 1.4483\n",
      "Epoch [2980/10000], Loss: 1.4376\n",
      "Epoch [2990/10000], Loss: 1.4271\n",
      "Epoch [3000/10000], Loss: 1.4166\n",
      "Epoch [3010/10000], Loss: 1.4061\n",
      "Epoch [3020/10000], Loss: 1.3955\n",
      "Epoch [3030/10000], Loss: 1.3850\n",
      "Epoch [3040/10000], Loss: 1.3746\n",
      "Epoch [3050/10000], Loss: 1.3644\n",
      "Epoch [3060/10000], Loss: 1.3544\n",
      "Epoch [3070/10000], Loss: 1.3443\n",
      "Epoch [3080/10000], Loss: 1.3343\n",
      "Epoch [3090/10000], Loss: 1.3246\n",
      "Epoch [3100/10000], Loss: 1.3150\n",
      "Epoch [3110/10000], Loss: 1.3054\n",
      "Epoch [3120/10000], Loss: 1.2959\n",
      "Epoch [3130/10000], Loss: 1.2863\n",
      "Epoch [3140/10000], Loss: 1.2768\n",
      "Epoch [3150/10000], Loss: 1.2673\n",
      "Epoch [3160/10000], Loss: 1.2578\n",
      "Epoch [3170/10000], Loss: 1.2482\n",
      "Epoch [3180/10000], Loss: 1.2389\n",
      "Epoch [3190/10000], Loss: 1.2297\n",
      "Epoch [3200/10000], Loss: 1.2205\n",
      "Epoch [3210/10000], Loss: 1.2115\n",
      "Epoch [3220/10000], Loss: 1.2028\n",
      "Epoch [3230/10000], Loss: 1.1943\n",
      "Epoch [3240/10000], Loss: 1.1857\n",
      "Epoch [3250/10000], Loss: 1.1772\n",
      "Epoch [3260/10000], Loss: 1.1687\n",
      "Epoch [3270/10000], Loss: 1.1604\n",
      "Epoch [3280/10000], Loss: 1.1522\n",
      "Epoch [3290/10000], Loss: 1.1439\n",
      "Epoch [3300/10000], Loss: 1.1354\n",
      "Epoch [3310/10000], Loss: 1.1271\n",
      "Epoch [3320/10000], Loss: 1.1189\n",
      "Epoch [3330/10000], Loss: 1.1106\n",
      "Epoch [3340/10000], Loss: 1.1022\n",
      "Epoch [3350/10000], Loss: 1.0939\n",
      "Epoch [3360/10000], Loss: 1.0857\n",
      "Epoch [3370/10000], Loss: 1.0775\n",
      "Epoch [3380/10000], Loss: 1.0694\n",
      "Epoch [3390/10000], Loss: 1.0615\n",
      "Epoch [3400/10000], Loss: 1.0537\n",
      "Epoch [3410/10000], Loss: 1.0460\n",
      "Epoch [3420/10000], Loss: 1.0384\n",
      "Epoch [3430/10000], Loss: 1.0309\n",
      "Epoch [3440/10000], Loss: 1.0233\n",
      "Epoch [3450/10000], Loss: 1.0159\n",
      "Epoch [3460/10000], Loss: 1.0085\n",
      "Epoch [3470/10000], Loss: 1.0012\n",
      "Epoch [3480/10000], Loss: 0.9941\n",
      "Epoch [3490/10000], Loss: 0.9870\n",
      "Epoch [3500/10000], Loss: 0.9800\n",
      "Epoch [3510/10000], Loss: 0.9730\n",
      "Epoch [3520/10000], Loss: 0.9661\n",
      "Epoch [3530/10000], Loss: 0.9593\n",
      "Epoch [3540/10000], Loss: 0.9526\n",
      "Epoch [3550/10000], Loss: 0.9460\n",
      "Epoch [3560/10000], Loss: 0.9394\n",
      "Epoch [3570/10000], Loss: 0.9330\n",
      "Epoch [3580/10000], Loss: 0.9266\n",
      "Epoch [3590/10000], Loss: 0.9203\n",
      "Epoch [3600/10000], Loss: 0.9141\n",
      "Epoch [3610/10000], Loss: 0.9079\n",
      "Epoch [3620/10000], Loss: 0.9016\n",
      "Epoch [3630/10000], Loss: 0.8953\n",
      "Epoch [3640/10000], Loss: 0.8889\n",
      "Epoch [3650/10000], Loss: 0.8825\n",
      "Epoch [3660/10000], Loss: 0.8762\n",
      "Epoch [3670/10000], Loss: 0.8699\n",
      "Epoch [3680/10000], Loss: 0.8637\n",
      "Epoch [3690/10000], Loss: 0.8575\n",
      "Epoch [3700/10000], Loss: 0.8513\n",
      "Epoch [3710/10000], Loss: 0.8449\n",
      "Epoch [3720/10000], Loss: 0.8383\n",
      "Epoch [3730/10000], Loss: 0.8316\n",
      "Epoch [3740/10000], Loss: 0.8250\n",
      "Epoch [3750/10000], Loss: 0.8186\n",
      "Epoch [3760/10000], Loss: 0.8123\n",
      "Epoch [3770/10000], Loss: 0.8060\n",
      "Epoch [3780/10000], Loss: 0.7999\n",
      "Epoch [3790/10000], Loss: 0.7939\n",
      "Epoch [3800/10000], Loss: 0.7879\n",
      "Epoch [3810/10000], Loss: 0.7819\n",
      "Epoch [3820/10000], Loss: 0.7760\n",
      "Epoch [3830/10000], Loss: 0.7702\n",
      "Epoch [3840/10000], Loss: 0.7645\n",
      "Epoch [3850/10000], Loss: 0.7588\n",
      "Epoch [3860/10000], Loss: 0.7532\n",
      "Epoch [3870/10000], Loss: 0.7477\n",
      "Epoch [3880/10000], Loss: 0.7423\n",
      "Epoch [3890/10000], Loss: 0.7370\n",
      "Epoch [3900/10000], Loss: 0.7317\n",
      "Epoch [3910/10000], Loss: 0.7265\n",
      "Epoch [3920/10000], Loss: 0.7214\n",
      "Epoch [3930/10000], Loss: 0.7164\n",
      "Epoch [3940/10000], Loss: 0.7115\n",
      "Epoch [3950/10000], Loss: 0.7065\n",
      "Epoch [3960/10000], Loss: 0.7016\n",
      "Epoch [3970/10000], Loss: 0.6968\n",
      "Epoch [3980/10000], Loss: 0.6921\n",
      "Epoch [3990/10000], Loss: 0.6873\n",
      "Epoch [4000/10000], Loss: 0.6826\n",
      "Epoch [4010/10000], Loss: 0.6779\n",
      "Epoch [4020/10000], Loss: 0.6733\n",
      "Epoch [4030/10000], Loss: 0.6689\n",
      "Epoch [4040/10000], Loss: 0.6645\n",
      "Epoch [4050/10000], Loss: 0.6603\n",
      "Epoch [4060/10000], Loss: 0.6562\n",
      "Epoch [4070/10000], Loss: 0.6523\n",
      "Epoch [4080/10000], Loss: 0.6483\n",
      "Epoch [4090/10000], Loss: 0.6445\n",
      "Epoch [4100/10000], Loss: 0.6407\n",
      "Epoch [4110/10000], Loss: 0.6370\n",
      "Epoch [4120/10000], Loss: 0.6333\n",
      "Epoch [4130/10000], Loss: 0.6295\n",
      "Epoch [4140/10000], Loss: 0.6258\n",
      "Epoch [4150/10000], Loss: 0.6220\n",
      "Epoch [4160/10000], Loss: 0.6183\n",
      "Epoch [4170/10000], Loss: 0.6145\n",
      "Epoch [4180/10000], Loss: 0.6107\n",
      "Epoch [4190/10000], Loss: 0.6069\n",
      "Epoch [4200/10000], Loss: 0.6033\n",
      "Epoch [4210/10000], Loss: 0.5996\n",
      "Epoch [4220/10000], Loss: 0.5960\n",
      "Epoch [4230/10000], Loss: 0.5924\n",
      "Epoch [4240/10000], Loss: 0.5888\n",
      "Epoch [4250/10000], Loss: 0.5852\n",
      "Epoch [4260/10000], Loss: 0.5815\n",
      "Epoch [4270/10000], Loss: 0.5778\n",
      "Epoch [4280/10000], Loss: 0.5740\n",
      "Epoch [4290/10000], Loss: 0.5702\n",
      "Epoch [4300/10000], Loss: 0.5665\n",
      "Epoch [4310/10000], Loss: 0.5628\n",
      "Epoch [4320/10000], Loss: 0.5593\n",
      "Epoch [4330/10000], Loss: 0.5558\n",
      "Epoch [4340/10000], Loss: 0.5523\n",
      "Epoch [4350/10000], Loss: 0.5486\n",
      "Epoch [4360/10000], Loss: 0.5449\n",
      "Epoch [4370/10000], Loss: 0.5413\n",
      "Epoch [4380/10000], Loss: 0.5377\n",
      "Epoch [4390/10000], Loss: 0.5340\n",
      "Epoch [4400/10000], Loss: 0.5303\n",
      "Epoch [4410/10000], Loss: 0.5265\n",
      "Epoch [4420/10000], Loss: 0.5227\n",
      "Epoch [4430/10000], Loss: 0.5190\n",
      "Epoch [4440/10000], Loss: 0.5152\n",
      "Epoch [4450/10000], Loss: 0.5116\n",
      "Epoch [4460/10000], Loss: 0.5080\n",
      "Epoch [4470/10000], Loss: 0.5044\n",
      "Epoch [4480/10000], Loss: 0.5009\n",
      "Epoch [4490/10000], Loss: 0.4974\n",
      "Epoch [4500/10000], Loss: 0.4939\n",
      "Epoch [4510/10000], Loss: 0.4904\n",
      "Epoch [4520/10000], Loss: 0.4871\n",
      "Epoch [4530/10000], Loss: 0.4838\n",
      "Epoch [4540/10000], Loss: 0.4805\n",
      "Epoch [4550/10000], Loss: 0.4773\n",
      "Epoch [4560/10000], Loss: 0.4740\n",
      "Epoch [4570/10000], Loss: 0.4707\n",
      "Epoch [4580/10000], Loss: 0.4672\n",
      "Epoch [4590/10000], Loss: 0.4637\n",
      "Epoch [4600/10000], Loss: 0.4604\n",
      "Epoch [4610/10000], Loss: 0.4571\n",
      "Epoch [4620/10000], Loss: 0.4538\n",
      "Epoch [4630/10000], Loss: 0.4505\n",
      "Epoch [4640/10000], Loss: 0.4473\n",
      "Epoch [4650/10000], Loss: 0.4442\n",
      "Epoch [4660/10000], Loss: 0.4411\n",
      "Epoch [4670/10000], Loss: 0.4382\n",
      "Epoch [4680/10000], Loss: 0.4353\n",
      "Epoch [4690/10000], Loss: 0.4324\n",
      "Epoch [4700/10000], Loss: 0.4297\n",
      "Epoch [4710/10000], Loss: 0.4270\n",
      "Epoch [4720/10000], Loss: 0.4244\n",
      "Epoch [4730/10000], Loss: 0.4218\n",
      "Epoch [4740/10000], Loss: 0.4192\n",
      "Epoch [4750/10000], Loss: 0.4166\n",
      "Epoch [4760/10000], Loss: 0.4140\n",
      "Epoch [4770/10000], Loss: 0.4115\n",
      "Epoch [4780/10000], Loss: 0.4091\n",
      "Epoch [4790/10000], Loss: 0.4068\n",
      "Epoch [4800/10000], Loss: 0.4043\n",
      "Epoch [4810/10000], Loss: 0.4018\n",
      "Epoch [4820/10000], Loss: 0.3992\n",
      "Epoch [4830/10000], Loss: 0.3966\n",
      "Epoch [4840/10000], Loss: 0.3941\n",
      "Epoch [4850/10000], Loss: 0.3917\n",
      "Epoch [4860/10000], Loss: 0.3893\n",
      "Epoch [4870/10000], Loss: 0.3870\n",
      "Epoch [4880/10000], Loss: 0.3848\n",
      "Epoch [4890/10000], Loss: 0.3825\n",
      "Epoch [4900/10000], Loss: 0.3804\n",
      "Epoch [4910/10000], Loss: 0.3782\n",
      "Epoch [4920/10000], Loss: 0.3761\n",
      "Epoch [4930/10000], Loss: 0.3741\n",
      "Epoch [4940/10000], Loss: 0.3720\n",
      "Epoch [4950/10000], Loss: 0.3700\n",
      "Epoch [4960/10000], Loss: 0.3680\n",
      "Epoch [4970/10000], Loss: 0.3661\n",
      "Epoch [4980/10000], Loss: 0.3641\n",
      "Epoch [4990/10000], Loss: 0.3623\n",
      "Epoch [5000/10000], Loss: 0.3604\n",
      "Epoch [5010/10000], Loss: 0.3585\n",
      "Epoch [5020/10000], Loss: 0.3567\n",
      "Epoch [5030/10000], Loss: 0.3547\n",
      "Epoch [5040/10000], Loss: 0.3528\n",
      "Epoch [5050/10000], Loss: 0.3508\n",
      "Epoch [5060/10000], Loss: 0.3489\n",
      "Epoch [5070/10000], Loss: 0.3469\n",
      "Epoch [5080/10000], Loss: 0.3450\n",
      "Epoch [5090/10000], Loss: 0.3431\n",
      "Epoch [5100/10000], Loss: 0.3412\n",
      "Epoch [5110/10000], Loss: 0.3393\n",
      "Epoch [5120/10000], Loss: 0.3374\n",
      "Epoch [5130/10000], Loss: 0.3356\n",
      "Epoch [5140/10000], Loss: 0.3338\n",
      "Epoch [5150/10000], Loss: 0.3320\n",
      "Epoch [5160/10000], Loss: 0.3303\n",
      "Epoch [5170/10000], Loss: 0.3285\n",
      "Epoch [5180/10000], Loss: 0.3268\n",
      "Epoch [5190/10000], Loss: 0.3252\n",
      "Epoch [5200/10000], Loss: 0.3235\n",
      "Epoch [5210/10000], Loss: 0.3219\n",
      "Epoch [5220/10000], Loss: 0.3202\n",
      "Epoch [5230/10000], Loss: 0.3186\n",
      "Epoch [5240/10000], Loss: 0.3169\n",
      "Epoch [5250/10000], Loss: 0.3153\n",
      "Epoch [5260/10000], Loss: 0.3136\n",
      "Epoch [5270/10000], Loss: 0.3120\n",
      "Epoch [5280/10000], Loss: 0.3105\n",
      "Epoch [5290/10000], Loss: 0.3089\n",
      "Epoch [5300/10000], Loss: 0.3074\n",
      "Epoch [5310/10000], Loss: 0.3057\n",
      "Epoch [5320/10000], Loss: 0.3040\n",
      "Epoch [5330/10000], Loss: 0.3024\n",
      "Epoch [5340/10000], Loss: 0.3007\n",
      "Epoch [5350/10000], Loss: 0.2991\n",
      "Epoch [5360/10000], Loss: 0.2976\n",
      "Epoch [5370/10000], Loss: 0.2961\n",
      "Epoch [5380/10000], Loss: 0.2946\n",
      "Epoch [5390/10000], Loss: 0.2931\n",
      "Epoch [5400/10000], Loss: 0.2916\n",
      "Epoch [5410/10000], Loss: 0.2901\n",
      "Epoch [5420/10000], Loss: 0.2887\n",
      "Epoch [5430/10000], Loss: 0.2872\n",
      "Epoch [5440/10000], Loss: 0.2857\n",
      "Epoch [5450/10000], Loss: 0.2843\n",
      "Epoch [5460/10000], Loss: 0.2828\n",
      "Epoch [5470/10000], Loss: 0.2814\n",
      "Epoch [5480/10000], Loss: 0.2801\n",
      "Epoch [5490/10000], Loss: 0.2787\n",
      "Epoch [5500/10000], Loss: 0.2774\n",
      "Epoch [5510/10000], Loss: 0.2760\n",
      "Epoch [5520/10000], Loss: 0.2747\n",
      "Epoch [5530/10000], Loss: 0.2734\n",
      "Epoch [5540/10000], Loss: 0.2721\n",
      "Epoch [5550/10000], Loss: 0.2708\n",
      "Epoch [5560/10000], Loss: 0.2694\n",
      "Epoch [5570/10000], Loss: 0.2680\n",
      "Epoch [5580/10000], Loss: 0.2666\n",
      "Epoch [5590/10000], Loss: 0.2652\n",
      "Epoch [5600/10000], Loss: 0.2638\n",
      "Epoch [5610/10000], Loss: 0.2625\n",
      "Epoch [5620/10000], Loss: 0.2611\n",
      "Epoch [5630/10000], Loss: 0.2598\n",
      "Epoch [5640/10000], Loss: 0.2585\n",
      "Epoch [5650/10000], Loss: 0.2572\n",
      "Epoch [5660/10000], Loss: 0.2559\n",
      "Epoch [5670/10000], Loss: 0.2547\n",
      "Epoch [5680/10000], Loss: 0.2534\n",
      "Epoch [5690/10000], Loss: 0.2522\n",
      "Epoch [5700/10000], Loss: 0.2510\n",
      "Epoch [5710/10000], Loss: 0.2499\n",
      "Epoch [5720/10000], Loss: 0.2488\n",
      "Epoch [5730/10000], Loss: 0.2474\n",
      "Epoch [5740/10000], Loss: 0.2461\n",
      "Epoch [5750/10000], Loss: 0.2448\n",
      "Epoch [5760/10000], Loss: 0.2435\n",
      "Epoch [5770/10000], Loss: 0.2423\n",
      "Epoch [5780/10000], Loss: 0.2411\n",
      "Epoch [5790/10000], Loss: 0.2399\n",
      "Epoch [5800/10000], Loss: 0.2387\n",
      "Epoch [5810/10000], Loss: 0.2375\n",
      "Epoch [5820/10000], Loss: 0.2364\n",
      "Epoch [5830/10000], Loss: 0.2352\n",
      "Epoch [5840/10000], Loss: 0.2341\n",
      "Epoch [5850/10000], Loss: 0.2330\n",
      "Epoch [5860/10000], Loss: 0.2319\n",
      "Epoch [5870/10000], Loss: 0.2308\n",
      "Epoch [5880/10000], Loss: 0.2297\n",
      "Epoch [5890/10000], Loss: 0.2286\n",
      "Epoch [5900/10000], Loss: 0.2276\n",
      "Epoch [5910/10000], Loss: 0.2265\n",
      "Epoch [5920/10000], Loss: 0.2255\n",
      "Epoch [5930/10000], Loss: 0.2245\n",
      "Epoch [5940/10000], Loss: 0.2235\n",
      "Epoch [5950/10000], Loss: 0.2224\n",
      "Epoch [5960/10000], Loss: 0.2214\n",
      "Epoch [5970/10000], Loss: 0.2204\n",
      "Epoch [5980/10000], Loss: 0.2194\n",
      "Epoch [5990/10000], Loss: 0.2185\n",
      "Epoch [6000/10000], Loss: 0.2175\n",
      "Epoch [6010/10000], Loss: 0.2166\n",
      "Epoch [6020/10000], Loss: 0.2156\n",
      "Epoch [6030/10000], Loss: 0.2146\n",
      "Epoch [6040/10000], Loss: 0.2137\n",
      "Epoch [6050/10000], Loss: 0.2127\n",
      "Epoch [6060/10000], Loss: 0.2116\n",
      "Epoch [6070/10000], Loss: 0.2105\n",
      "Epoch [6080/10000], Loss: 0.2095\n",
      "Epoch [6090/10000], Loss: 0.2085\n",
      "Epoch [6100/10000], Loss: 0.2075\n",
      "Epoch [6110/10000], Loss: 0.2065\n",
      "Epoch [6120/10000], Loss: 0.2055\n",
      "Epoch [6130/10000], Loss: 0.2046\n",
      "Epoch [6140/10000], Loss: 0.2036\n",
      "Epoch [6150/10000], Loss: 0.2027\n",
      "Epoch [6160/10000], Loss: 0.2017\n",
      "Epoch [6170/10000], Loss: 0.2008\n",
      "Epoch [6180/10000], Loss: 0.1997\n",
      "Epoch [6190/10000], Loss: 0.1986\n",
      "Epoch [6200/10000], Loss: 0.1975\n",
      "Epoch [6210/10000], Loss: 0.1965\n",
      "Epoch [6220/10000], Loss: 0.1953\n",
      "Epoch [6230/10000], Loss: 0.1942\n",
      "Epoch [6240/10000], Loss: 0.1932\n",
      "Epoch [6250/10000], Loss: 0.1922\n",
      "Epoch [6260/10000], Loss: 0.1912\n",
      "Epoch [6270/10000], Loss: 0.1903\n",
      "Epoch [6280/10000], Loss: 0.1893\n",
      "Epoch [6290/10000], Loss: 0.1883\n",
      "Epoch [6300/10000], Loss: 0.1874\n",
      "Epoch [6310/10000], Loss: 0.1864\n",
      "Epoch [6320/10000], Loss: 0.1855\n",
      "Epoch [6330/10000], Loss: 0.1846\n",
      "Epoch [6340/10000], Loss: 0.1837\n",
      "Epoch [6350/10000], Loss: 0.1828\n",
      "Epoch [6360/10000], Loss: 0.1819\n",
      "Epoch [6370/10000], Loss: 0.1810\n",
      "Epoch [6380/10000], Loss: 0.1802\n",
      "Epoch [6390/10000], Loss: 0.1793\n",
      "Epoch [6400/10000], Loss: 0.1784\n",
      "Epoch [6410/10000], Loss: 0.1776\n",
      "Epoch [6420/10000], Loss: 0.1767\n",
      "Epoch [6430/10000], Loss: 0.1759\n",
      "Epoch [6440/10000], Loss: 0.1751\n",
      "Epoch [6450/10000], Loss: 0.1742\n",
      "Epoch [6460/10000], Loss: 0.1733\n",
      "Epoch [6470/10000], Loss: 0.1722\n",
      "Epoch [6480/10000], Loss: 0.1711\n",
      "Epoch [6490/10000], Loss: 0.1701\n",
      "Epoch [6500/10000], Loss: 0.1691\n",
      "Epoch [6510/10000], Loss: 0.1681\n",
      "Epoch [6520/10000], Loss: 0.1671\n",
      "Epoch [6530/10000], Loss: 0.1662\n",
      "Epoch [6540/10000], Loss: 0.1653\n",
      "Epoch [6550/10000], Loss: 0.1644\n",
      "Epoch [6560/10000], Loss: 0.1635\n",
      "Epoch [6570/10000], Loss: 0.1627\n",
      "Epoch [6580/10000], Loss: 0.1619\n",
      "Epoch [6590/10000], Loss: 0.1610\n",
      "Epoch [6600/10000], Loss: 0.1602\n",
      "Epoch [6610/10000], Loss: 0.1593\n",
      "Epoch [6620/10000], Loss: 0.1585\n",
      "Epoch [6630/10000], Loss: 0.1575\n",
      "Epoch [6640/10000], Loss: 0.1565\n",
      "Epoch [6650/10000], Loss: 0.1555\n",
      "Epoch [6660/10000], Loss: 0.1546\n",
      "Epoch [6670/10000], Loss: 0.1536\n",
      "Epoch [6680/10000], Loss: 0.1527\n",
      "Epoch [6690/10000], Loss: 0.1518\n",
      "Epoch [6700/10000], Loss: 0.1509\n",
      "Epoch [6710/10000], Loss: 0.1500\n",
      "Epoch [6720/10000], Loss: 0.1491\n",
      "Epoch [6730/10000], Loss: 0.1483\n",
      "Epoch [6740/10000], Loss: 0.1474\n",
      "Epoch [6750/10000], Loss: 0.1466\n",
      "Epoch [6760/10000], Loss: 0.1459\n",
      "Epoch [6770/10000], Loss: 0.1451\n",
      "Epoch [6780/10000], Loss: 0.1444\n",
      "Epoch [6790/10000], Loss: 0.1436\n",
      "Epoch [6800/10000], Loss: 0.1429\n",
      "Epoch [6810/10000], Loss: 0.1422\n",
      "Epoch [6820/10000], Loss: 0.1414\n",
      "Epoch [6830/10000], Loss: 0.1407\n",
      "Epoch [6840/10000], Loss: 0.1400\n",
      "Epoch [6850/10000], Loss: 0.1393\n",
      "Epoch [6860/10000], Loss: 0.1386\n",
      "Epoch [6870/10000], Loss: 0.1379\n",
      "Epoch [6880/10000], Loss: 0.1372\n",
      "Epoch [6890/10000], Loss: 0.1365\n",
      "Epoch [6900/10000], Loss: 0.1359\n",
      "Epoch [6910/10000], Loss: 0.1352\n",
      "Epoch [6920/10000], Loss: 0.1345\n",
      "Epoch [6930/10000], Loss: 0.1339\n",
      "Epoch [6940/10000], Loss: 0.1332\n",
      "Epoch [6950/10000], Loss: 0.1326\n",
      "Epoch [6960/10000], Loss: 0.1319\n",
      "Epoch [6970/10000], Loss: 0.1313\n",
      "Epoch [6980/10000], Loss: 0.1306\n",
      "Epoch [6990/10000], Loss: 0.1299\n",
      "Epoch [7000/10000], Loss: 0.1292\n",
      "Epoch [7010/10000], Loss: 0.1285\n",
      "Epoch [7020/10000], Loss: 0.1278\n",
      "Epoch [7030/10000], Loss: 0.1271\n",
      "Epoch [7040/10000], Loss: 0.1265\n",
      "Epoch [7050/10000], Loss: 0.1259\n",
      "Epoch [7060/10000], Loss: 0.1252\n",
      "Epoch [7070/10000], Loss: 0.1246\n",
      "Epoch [7080/10000], Loss: 0.1239\n",
      "Epoch [7090/10000], Loss: 0.1233\n",
      "Epoch [7100/10000], Loss: 0.1226\n",
      "Epoch [7110/10000], Loss: 0.1220\n",
      "Epoch [7120/10000], Loss: 0.1214\n",
      "Epoch [7130/10000], Loss: 0.1207\n",
      "Epoch [7140/10000], Loss: 0.1200\n",
      "Epoch [7150/10000], Loss: 0.1193\n",
      "Epoch [7160/10000], Loss: 0.1186\n",
      "Epoch [7170/10000], Loss: 0.1179\n",
      "Epoch [7180/10000], Loss: 0.1173\n",
      "Epoch [7190/10000], Loss: 0.1166\n",
      "Epoch [7200/10000], Loss: 0.1160\n",
      "Epoch [7210/10000], Loss: 0.1153\n",
      "Epoch [7220/10000], Loss: 0.1145\n",
      "Epoch [7230/10000], Loss: 0.1138\n",
      "Epoch [7240/10000], Loss: 0.1131\n",
      "Epoch [7250/10000], Loss: 0.1123\n",
      "Epoch [7260/10000], Loss: 0.1115\n",
      "Epoch [7270/10000], Loss: 0.1107\n",
      "Epoch [7280/10000], Loss: 0.1099\n",
      "Epoch [7290/10000], Loss: 0.1092\n",
      "Epoch [7300/10000], Loss: 0.1085\n",
      "Epoch [7310/10000], Loss: 0.1077\n",
      "Epoch [7320/10000], Loss: 0.1071\n",
      "Epoch [7330/10000], Loss: 0.1064\n",
      "Epoch [7340/10000], Loss: 0.1060\n",
      "Epoch [7350/10000], Loss: 0.1066\n",
      "Epoch [7360/10000], Loss: 0.1048\n",
      "Epoch [7370/10000], Loss: 0.1040\n",
      "Epoch [7380/10000], Loss: 0.1034\n",
      "Epoch [7390/10000], Loss: 0.1029\n",
      "Epoch [7400/10000], Loss: 0.1023\n",
      "Epoch [7410/10000], Loss: 0.1017\n",
      "Epoch [7420/10000], Loss: 0.1012\n",
      "Epoch [7430/10000], Loss: 0.1012\n",
      "Epoch [7440/10000], Loss: 0.1013\n",
      "Epoch [7450/10000], Loss: 0.0999\n",
      "Epoch [7460/10000], Loss: 0.0990\n",
      "Epoch [7470/10000], Loss: 0.0983\n",
      "Epoch [7480/10000], Loss: 0.0976\n",
      "Epoch [7490/10000], Loss: 0.0970\n",
      "Epoch [7500/10000], Loss: 0.0963\n",
      "Epoch [7510/10000], Loss: 0.0957\n",
      "Epoch [7520/10000], Loss: 0.0953\n",
      "Epoch [7530/10000], Loss: 0.0985\n",
      "Epoch [7540/10000], Loss: 0.0953\n",
      "Epoch [7550/10000], Loss: 0.0935\n",
      "Epoch [7560/10000], Loss: 0.0926\n",
      "Epoch [7570/10000], Loss: 0.0920\n",
      "Epoch [7580/10000], Loss: 0.0914\n",
      "Epoch [7590/10000], Loss: 0.0909\n",
      "Epoch [7600/10000], Loss: 0.0903\n",
      "Epoch [7610/10000], Loss: 0.0897\n",
      "Epoch [7620/10000], Loss: 0.0891\n",
      "Epoch [7630/10000], Loss: 0.0885\n",
      "Epoch [7640/10000], Loss: 0.0880\n",
      "Epoch [7650/10000], Loss: 0.0874\n",
      "Epoch [7660/10000], Loss: 0.0869\n",
      "Epoch [7670/10000], Loss: 0.0873\n",
      "Epoch [7680/10000], Loss: 0.0875\n",
      "Epoch [7690/10000], Loss: 0.0856\n",
      "Epoch [7700/10000], Loss: 0.0853\n",
      "Epoch [7710/10000], Loss: 0.0842\n",
      "Epoch [7720/10000], Loss: 0.0838\n",
      "Epoch [7730/10000], Loss: 0.0832\n",
      "Epoch [7740/10000], Loss: 0.0826\n",
      "Epoch [7750/10000], Loss: 0.0820\n",
      "Epoch [7760/10000], Loss: 0.0814\n",
      "Epoch [7770/10000], Loss: 0.0809\n",
      "Epoch [7780/10000], Loss: 0.0804\n",
      "Epoch [7790/10000], Loss: 0.0798\n",
      "Epoch [7800/10000], Loss: 0.0793\n",
      "Epoch [7810/10000], Loss: 0.0788\n",
      "Epoch [7820/10000], Loss: 0.0783\n",
      "Epoch [7830/10000], Loss: 0.0779\n",
      "Epoch [7840/10000], Loss: 0.0776\n",
      "Epoch [7850/10000], Loss: 0.0839\n",
      "Epoch [7860/10000], Loss: 0.0796\n",
      "Epoch [7870/10000], Loss: 0.0760\n",
      "Epoch [7880/10000], Loss: 0.0759\n",
      "Epoch [7890/10000], Loss: 0.0751\n",
      "Epoch [7900/10000], Loss: 0.0746\n",
      "Epoch [7910/10000], Loss: 0.0741\n",
      "Epoch [7920/10000], Loss: 0.0736\n",
      "Epoch [7930/10000], Loss: 0.0732\n",
      "Epoch [7940/10000], Loss: 0.0727\n",
      "Epoch [7950/10000], Loss: 0.0723\n",
      "Epoch [7960/10000], Loss: 0.0719\n",
      "Epoch [7970/10000], Loss: 0.0715\n",
      "Epoch [7980/10000], Loss: 0.0712\n",
      "Epoch [7990/10000], Loss: 0.0708\n",
      "Epoch [8000/10000], Loss: 0.0704\n",
      "Epoch [8010/10000], Loss: 0.0701\n",
      "Epoch [8020/10000], Loss: 0.0698\n",
      "Epoch [8030/10000], Loss: 0.0701\n",
      "Epoch [8040/10000], Loss: 0.0772\n",
      "Epoch [8050/10000], Loss: 0.0705\n",
      "Epoch [8060/10000], Loss: 0.0685\n",
      "Epoch [8070/10000], Loss: 0.0684\n",
      "Epoch [8080/10000], Loss: 0.0679\n",
      "Epoch [8090/10000], Loss: 0.0675\n",
      "Epoch [8100/10000], Loss: 0.0672\n",
      "Epoch [8110/10000], Loss: 0.0669\n",
      "Epoch [8120/10000], Loss: 0.0666\n",
      "Epoch [8130/10000], Loss: 0.0663\n",
      "Epoch [8140/10000], Loss: 0.0660\n",
      "Epoch [8150/10000], Loss: 0.0657\n",
      "Epoch [8160/10000], Loss: 0.0654\n",
      "Epoch [8170/10000], Loss: 0.0652\n",
      "Epoch [8180/10000], Loss: 0.0649\n",
      "Epoch [8190/10000], Loss: 0.0646\n",
      "Epoch [8200/10000], Loss: 0.0643\n",
      "Epoch [8210/10000], Loss: 0.0640\n",
      "Epoch [8220/10000], Loss: 0.0637\n",
      "Epoch [8230/10000], Loss: 0.0634\n",
      "Epoch [8240/10000], Loss: 0.0631\n",
      "Epoch [8250/10000], Loss: 0.0634\n",
      "Epoch [8260/10000], Loss: 0.0705\n",
      "Epoch [8270/10000], Loss: 0.0645\n",
      "Epoch [8280/10000], Loss: 0.0621\n",
      "Epoch [8290/10000], Loss: 0.0617\n",
      "Epoch [8300/10000], Loss: 0.0615\n",
      "Epoch [8310/10000], Loss: 0.0612\n",
      "Epoch [8320/10000], Loss: 0.0610\n",
      "Epoch [8330/10000], Loss: 0.0607\n",
      "Epoch [8340/10000], Loss: 0.0605\n",
      "Epoch [8350/10000], Loss: 0.0603\n",
      "Epoch [8360/10000], Loss: 0.0601\n",
      "Epoch [8370/10000], Loss: 0.0599\n",
      "Epoch [8380/10000], Loss: 0.0597\n",
      "Epoch [8390/10000], Loss: 0.0595\n",
      "Epoch [8400/10000], Loss: 0.0597\n",
      "Epoch [8410/10000], Loss: 0.0704\n",
      "Epoch [8420/10000], Loss: 0.0618\n",
      "Epoch [8430/10000], Loss: 0.0586\n",
      "Epoch [8440/10000], Loss: 0.0589\n",
      "Epoch [8450/10000], Loss: 0.0583\n",
      "Epoch [8460/10000], Loss: 0.0580\n",
      "Epoch [8470/10000], Loss: 0.0578\n",
      "Epoch [8480/10000], Loss: 0.0576\n",
      "Epoch [8490/10000], Loss: 0.0574\n",
      "Epoch [8500/10000], Loss: 0.0571\n",
      "Epoch [8510/10000], Loss: 0.0569\n",
      "Epoch [8520/10000], Loss: 0.0567\n",
      "Epoch [8530/10000], Loss: 0.0565\n",
      "Epoch [8540/10000], Loss: 0.0563\n",
      "Epoch [8550/10000], Loss: 0.0560\n",
      "Epoch [8560/10000], Loss: 0.0558\n",
      "Epoch [8570/10000], Loss: 0.0581\n",
      "Epoch [8580/10000], Loss: 0.0567\n",
      "Epoch [8590/10000], Loss: 0.0550\n",
      "Epoch [8600/10000], Loss: 0.0552\n",
      "Epoch [8610/10000], Loss: 0.0548\n",
      "Epoch [8620/10000], Loss: 0.0545\n",
      "Epoch [8630/10000], Loss: 0.0542\n",
      "Epoch [8640/10000], Loss: 0.0539\n",
      "Epoch [8650/10000], Loss: 0.0537\n",
      "Epoch [8660/10000], Loss: 0.0535\n",
      "Epoch [8670/10000], Loss: 0.0533\n",
      "Epoch [8680/10000], Loss: 0.0530\n",
      "Epoch [8690/10000], Loss: 0.0528\n",
      "Epoch [8700/10000], Loss: 0.0527\n",
      "Epoch [8710/10000], Loss: 0.0549\n",
      "Epoch [8720/10000], Loss: 0.0548\n",
      "Epoch [8730/10000], Loss: 0.0521\n",
      "Epoch [8740/10000], Loss: 0.0517\n",
      "Epoch [8750/10000], Loss: 0.0516\n",
      "Epoch [8760/10000], Loss: 0.0513\n",
      "Epoch [8770/10000], Loss: 0.0511\n",
      "Epoch [8780/10000], Loss: 0.0509\n",
      "Epoch [8790/10000], Loss: 0.0507\n",
      "Epoch [8800/10000], Loss: 0.0504\n",
      "Epoch [8810/10000], Loss: 0.0502\n",
      "Epoch [8820/10000], Loss: 0.0500\n",
      "Epoch [8830/10000], Loss: 0.0499\n",
      "Epoch [8840/10000], Loss: 0.0528\n",
      "Epoch [8850/10000], Loss: 0.0510\n",
      "Epoch [8860/10000], Loss: 0.0495\n",
      "Epoch [8870/10000], Loss: 0.0489\n",
      "Epoch [8880/10000], Loss: 0.0487\n",
      "Epoch [8890/10000], Loss: 0.0485\n",
      "Epoch [8900/10000], Loss: 0.0483\n",
      "Epoch [8910/10000], Loss: 0.0481\n",
      "Epoch [8920/10000], Loss: 0.0479\n",
      "Epoch [8930/10000], Loss: 0.0477\n",
      "Epoch [8940/10000], Loss: 0.0475\n",
      "Epoch [8950/10000], Loss: 0.0473\n",
      "Epoch [8960/10000], Loss: 0.0479\n",
      "Epoch [8970/10000], Loss: 0.0627\n",
      "Epoch [8980/10000], Loss: 0.0510\n",
      "Epoch [8990/10000], Loss: 0.0465\n",
      "Epoch [9000/10000], Loss: 0.0467\n",
      "Epoch [9010/10000], Loss: 0.0464\n",
      "Epoch [9020/10000], Loss: 0.0460\n",
      "Epoch [9030/10000], Loss: 0.0458\n",
      "Epoch [9040/10000], Loss: 0.0456\n",
      "Epoch [9050/10000], Loss: 0.0454\n",
      "Epoch [9060/10000], Loss: 0.0452\n",
      "Epoch [9070/10000], Loss: 0.0450\n",
      "Epoch [9080/10000], Loss: 0.0448\n",
      "Epoch [9090/10000], Loss: 0.0447\n",
      "Epoch [9100/10000], Loss: 0.0445\n",
      "Epoch [9110/10000], Loss: 0.0445\n",
      "Epoch [9120/10000], Loss: 0.0493\n",
      "Epoch [9130/10000], Loss: 0.0445\n",
      "Epoch [9140/10000], Loss: 0.0440\n",
      "Epoch [9150/10000], Loss: 0.0442\n",
      "Epoch [9160/10000], Loss: 0.0439\n",
      "Epoch [9170/10000], Loss: 0.0435\n",
      "Epoch [9180/10000], Loss: 0.0433\n",
      "Epoch [9190/10000], Loss: 0.0431\n",
      "Epoch [9200/10000], Loss: 0.0430\n",
      "Epoch [9210/10000], Loss: 0.0429\n",
      "Epoch [9220/10000], Loss: 0.0427\n",
      "Epoch [9230/10000], Loss: 0.0426\n",
      "Epoch [9240/10000], Loss: 0.0425\n",
      "Epoch [9250/10000], Loss: 0.0425\n",
      "Epoch [9260/10000], Loss: 0.0512\n",
      "Epoch [9270/10000], Loss: 0.0433\n",
      "Epoch [9280/10000], Loss: 0.0453\n",
      "Epoch [9290/10000], Loss: 0.0423\n",
      "Epoch [9300/10000], Loss: 0.0417\n",
      "Epoch [9310/10000], Loss: 0.0417\n",
      "Epoch [9320/10000], Loss: 0.0415\n",
      "Epoch [9330/10000], Loss: 0.0413\n",
      "Epoch [9340/10000], Loss: 0.0412\n",
      "Epoch [9350/10000], Loss: 0.0411\n",
      "Epoch [9360/10000], Loss: 0.0410\n",
      "Epoch [9370/10000], Loss: 0.0409\n",
      "Epoch [9380/10000], Loss: 0.0408\n",
      "Epoch [9390/10000], Loss: 0.0407\n",
      "Epoch [9400/10000], Loss: 0.0406\n",
      "Epoch [9410/10000], Loss: 0.0406\n",
      "Epoch [9420/10000], Loss: 0.0449\n",
      "Epoch [9430/10000], Loss: 0.0440\n",
      "Epoch [9440/10000], Loss: 0.0406\n",
      "Epoch [9450/10000], Loss: 0.0417\n",
      "Epoch [9460/10000], Loss: 0.0403\n",
      "Epoch [9470/10000], Loss: 0.0399\n",
      "Epoch [9480/10000], Loss: 0.0398\n",
      "Epoch [9490/10000], Loss: 0.0397\n",
      "Epoch [9500/10000], Loss: 0.0396\n",
      "Epoch [9510/10000], Loss: 0.0395\n",
      "Epoch [9520/10000], Loss: 0.0394\n",
      "Epoch [9530/10000], Loss: 0.0393\n",
      "Epoch [9540/10000], Loss: 0.0392\n",
      "Epoch [9550/10000], Loss: 0.0391\n",
      "Epoch [9560/10000], Loss: 0.0390\n",
      "Epoch [9570/10000], Loss: 0.0390\n",
      "Epoch [9580/10000], Loss: 0.0428\n",
      "Epoch [9590/10000], Loss: 0.0475\n",
      "Epoch [9600/10000], Loss: 0.0392\n",
      "Epoch [9610/10000], Loss: 0.0410\n",
      "Epoch [9620/10000], Loss: 0.0386\n",
      "Epoch [9630/10000], Loss: 0.0385\n",
      "Epoch [9640/10000], Loss: 0.0384\n",
      "Epoch [9650/10000], Loss: 0.0382\n",
      "Epoch [9660/10000], Loss: 0.0381\n",
      "Epoch [9670/10000], Loss: 0.0380\n",
      "Epoch [9680/10000], Loss: 0.0379\n",
      "Epoch [9690/10000], Loss: 0.0378\n",
      "Epoch [9700/10000], Loss: 0.0378\n",
      "Epoch [9710/10000], Loss: 0.0377\n",
      "Epoch [9720/10000], Loss: 0.0376\n",
      "Epoch [9730/10000], Loss: 0.0375\n",
      "Epoch [9740/10000], Loss: 0.0375\n",
      "Epoch [9750/10000], Loss: 0.0414\n",
      "Epoch [9760/10000], Loss: 0.0488\n",
      "Epoch [9770/10000], Loss: 0.0372\n",
      "Epoch [9780/10000], Loss: 0.0387\n",
      "Epoch [9790/10000], Loss: 0.0378\n",
      "Epoch [9800/10000], Loss: 0.0370\n",
      "Epoch [9810/10000], Loss: 0.0368\n",
      "Epoch [9820/10000], Loss: 0.0367\n",
      "Epoch [9830/10000], Loss: 0.0367\n",
      "Epoch [9840/10000], Loss: 0.0366\n",
      "Epoch [9850/10000], Loss: 0.0365\n",
      "Epoch [9860/10000], Loss: 0.0364\n",
      "Epoch [9870/10000], Loss: 0.0363\n",
      "Epoch [9880/10000], Loss: 0.0363\n",
      "Epoch [9890/10000], Loss: 0.0362\n",
      "Epoch [9900/10000], Loss: 0.0362\n",
      "Epoch [9910/10000], Loss: 0.0408\n",
      "Epoch [9920/10000], Loss: 0.0474\n",
      "Epoch [9930/10000], Loss: 0.0361\n",
      "Epoch [9940/10000], Loss: 0.0385\n",
      "Epoch [9950/10000], Loss: 0.0361\n",
      "Epoch [9960/10000], Loss: 0.0357\n",
      "Epoch [9970/10000], Loss: 0.0357\n",
      "Epoch [9980/10000], Loss: 0.0356\n",
      "Epoch [9990/10000], Loss: 0.0354\n",
      "Epoch [10000/10000], Loss: 0.0354\n"
     ]
    }
   ],
   "source": [
    "# Training Loop \n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    reg_out, class_out = model(X_train_tensor)\n",
    "    \n",
    "    # Compute losses\n",
    "    loss_reg = criterion_reg(reg_out, y_reg_train_tensor)\n",
    "    loss_class = critreion_class(class_out, y_class_train_tensor)\n",
    "    \n",
    "    # Total loss\n",
    "    loss = loss_reg + loss_class\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41eb53b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Regression MSE: 1.1217\n",
      "Test Classification Accuracy: 0.9950\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_reg, y_pred_class = model(X_test_tensor)\n",
    "    \n",
    "    reg_mse = criterion_reg(y_pred_reg, y_reg_test_tensor)\n",
    "    class_acc = ((y_pred_class > 0.5) == y_class_test_tensor).float().mean()\n",
    "    \n",
    "print(f\"Test Regression MSE: {reg_mse.item():.4f}\")\n",
    "print(f\"Test Classification Accuracy: {class_acc.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
