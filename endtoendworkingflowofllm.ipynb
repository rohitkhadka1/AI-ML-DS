{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMZWjmphQT2w5jOGWaGGqO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyOj24QlK41u"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()  # Inference mode (no gradients)\n",
        "\n",
        "# Step 1: Input prompt\n",
        "prompt = \"What is the capital city of Nepal?\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "print(\"Token IDs:\", input_ids)\n",
        "\n",
        "# Step 2: Pass through model to get output logits\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    logits = outputs.logits  # Shape: [1, seq_len, vocab_size]\n",
        "\n",
        "# Step 3: Focus on the last token's logits\n",
        "last_token_logits = logits[0, -1, :]\n",
        "\n",
        "# Step 4: Apply temperature and top-k sampling\n",
        "temperature = 0.8\n",
        "top_k = 50\n",
        "\n",
        "# Scale logits by temperature\n",
        "scaled_logits = last_token_logits / temperature\n",
        "\n",
        "# Top-k filtering\n",
        "top_k_logits, top_k_indices = torch.topk(scaled_logits, top_k)\n",
        "top_k_probs = F.softmax(top_k_logits, dim=-1)\n",
        "\n",
        "# Sample from the filtered distribution\n",
        "next_token = torch.multinomial(top_k_probs, num_samples=1)\n",
        "next_token_id = top_k_indices[next_token]\n",
        "\n",
        "# Step 5: Append new token to input and repeat\n",
        "generated = input_ids.tolist()[0] + [next_token_id.item()]\n",
        "generated_tensor = torch.tensor([generated])\n",
        "\n",
        "# You can loop this process to generate more tokens:\n",
        "# for _ in range(num_tokens_to_generate):\n",
        "#     pass  # Repeat from Step 2 onwards with updated generated_tensor\n",
        "\n",
        "# Decode the tokens\n",
        "decoded_text = tokenizer.decode(generated_tensor[0])\n",
        "print(\"\\nGenerated Text:\")\n",
        "print(decoded_text)\n"
      ]
    }
  ]
}